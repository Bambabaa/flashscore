{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e63ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1628398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flashscore Soccer Match Data Scraper\n",
    "\n",
    "This script scrapes soccer match data from Flashscore using a functional approach,\n",
    "with separate functions for different scraping operations.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException,\n",
    "    NoSuchElementException,\n",
    "    WebDriverException,\n",
    "    StaleElementReferenceException\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce480428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global variables\n",
    "BASE_URL = \"https://www.flashscore.com\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"flashscore_scraper.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"flashscore_scraper\")\n",
    "\n",
    "# Initialize global variables for driver and session\n",
    "driver = None\n",
    "session = None\n",
    "\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"Create directories for storing data.\"\"\"\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    os.makedirs(\"data/matches\", exist_ok=True)\n",
    "    os.makedirs(\"data/leagues\", exist_ok=True)\n",
    "    os.makedirs(\"data/live\", exist_ok=True)\n",
    "\n",
    "\n",
    "def setup_selenium_driver(headless: bool = True, chrome_driver_path: Optional[str] = None) -> webdriver.Chrome:\n",
    "    \"\"\"\n",
    "    Set up the Selenium WebDriver.\n",
    "    \n",
    "    Args:\n",
    "        headless: Whether to run the browser in headless mode\n",
    "        chrome_driver_path: Path to the Chrome webdriver executable\n",
    "        \n",
    "    Returns:\n",
    "        A configured Chrome WebDriver\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless\")\n",
    "    \n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    options.add_argument(\"--disable-popup-blocking\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(f\"user-agent={HEADERS['User-Agent']}\")\n",
    "    \n",
    "    if chrome_driver_path:\n",
    "        service = Service(executable_path=chrome_driver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "    else:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def init_session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    Initialize a requests session with appropriate headers.\n",
    "    \n",
    "    Returns:\n",
    "        A configured requests Session\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "    return session\n",
    "\n",
    "\n",
    "def random_delay(min_seconds: float = 1.5, max_seconds: float = 4.0):\n",
    "    \"\"\"\n",
    "    Add a random delay to avoid detection.\n",
    "    \n",
    "    Args:\n",
    "        min_seconds: Minimum delay in seconds\n",
    "        max_seconds: Maximum delay in seconds\n",
    "    \"\"\"\n",
    "    delay = random.uniform(min_seconds, max_seconds)\n",
    "    time.sleep(delay)\n",
    "\n",
    "\n",
    "def get_page_content(url: str, use_selenium: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Get the HTML content of a page.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to fetch\n",
    "        use_selenium: Whether to use Selenium (True) or requests (False)\n",
    "        \n",
    "    Returns:\n",
    "        The HTML content of the page\n",
    "    \"\"\"\n",
    "    global driver, session\n",
    "    \n",
    "    random_delay()\n",
    "    \n",
    "    if use_selenium:\n",
    "        if not driver:\n",
    "            logger.error(\"Selenium driver not initialized\")\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            # Wait for the main content to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            return driver.page_source\n",
    "        except (TimeoutException, WebDriverException) as e:\n",
    "            logger.error(f\"Error loading page with Selenium: {url} - {str(e)}\")\n",
    "            return \"\"\n",
    "    else:\n",
    "        if not session:\n",
    "            logger.error(\"Requests session not initialized\")\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error loading page with requests: {url} - {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "def get_available_leagues(use_selenium: bool = True) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Get a list of available soccer leagues.\n",
    "    \n",
    "    Args:\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries containing league information\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/football/\"\n",
    "    content = get_page_content(url, use_selenium)\n",
    "    \n",
    "    if not content:\n",
    "        logger.error(\"Failed to fetch available leagues\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    leagues = []\n",
    "    \n",
    "    # This selector will need to be updated based on actual Flashscore HTML structure\n",
    "    # The following is a placeholder that needs to be adjusted\n",
    "    league_elements = soup.select(\".leagues-list a\")\n",
    "    \n",
    "    for element in league_elements:\n",
    "        league_url = element.get(\"href\", \"\")\n",
    "        if league_url:\n",
    "            league_name = element.text.strip()\n",
    "            league_id = league_url.split(\"/\")[-1]\n",
    "            \n",
    "            leagues.append({\n",
    "                \"id\": league_id,\n",
    "                \"name\": league_name,\n",
    "                \"url\": f\"{BASE_URL}{league_url}\"\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Found {len(leagues)} leagues\")\n",
    "    return leagues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44ec843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_league_fixtures(league_url: str, season: Optional[str] = None, use_selenium: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get fixtures for a specific league.\n",
    "    \n",
    "    Args:\n",
    "        league_url: The URL of the league\n",
    "        season: Optional season identifier\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries containing fixture information\n",
    "    \"\"\"\n",
    "    fixtures_url = f\"{league_url}/fixtures/\"\n",
    "    if season:\n",
    "        fixtures_url = f\"{fixtures_url}{season}/\"\n",
    "    \n",
    "    content = get_page_content(fixtures_url, use_selenium)\n",
    "    \n",
    "    if not content:\n",
    "        logger.error(f\"Failed to fetch fixtures for league: {league_url}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    fixtures = []\n",
    "    \n",
    "    # This selector will need to be updated based on actual Flashscore HTML structure\n",
    "    match_elements = soup.select(\".event__match\")\n",
    "    \n",
    "    for element in match_elements:\n",
    "        try:\n",
    "            match_id = element.get(\"id\", \"\").replace(\"g_1_\", \"\")\n",
    "            \n",
    "            if not match_id:\n",
    "                continue\n",
    "            \n",
    "            home_team = element.select_one(\".event__participant--home\").text.strip()\n",
    "            away_team = element.select_one(\".event__participant--away\").text.strip()\n",
    "            \n",
    "            match_time_element = element.select_one(\".event__time\")\n",
    "            match_date = match_time_element.get(\"data-date\", \"\") if match_time_element else \"\"\n",
    "            match_time = match_time_element.text.strip() if match_time_element else \"\"\n",
    "            \n",
    "            match_status_element = element.select_one(\".event__stage\")\n",
    "            match_status = match_status_element.text.strip() if match_status_element else \"\"\n",
    "            \n",
    "            score_element = element.select_one(\".event__scores\")\n",
    "            score = score_element.text.strip() if score_element else \"\"\n",
    "            \n",
    "            fixture = {\n",
    "                \"id\": match_id,\n",
    "                \"home_team\": home_team,\n",
    "                \"away_team\": away_team,\n",
    "                \"date\": match_date,\n",
    "                \"time\": match_time,\n",
    "                \"status\": match_status,\n",
    "                \"score\": score,\n",
    "                \"url\": f\"{BASE_URL}/match/{match_id}/\"\n",
    "            }\n",
    "            \n",
    "            fixtures.append(fixture)\n",
    "        except (AttributeError, Exception) as e:\n",
    "            logger.error(f\"Error parsing fixture: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Found {len(fixtures)} fixtures for league: {league_url}\")\n",
    "    return fixtures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4560324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_match_details(match_id: str, use_selenium: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get detailed information for a specific match.\n",
    "    \n",
    "    Args:\n",
    "        match_id: The ID of the match\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing match details\n",
    "    \"\"\"\n",
    "    match_url = f\"{BASE_URL}/match/{match_id}/\"\n",
    "    content = get_page_content(match_url, use_selenium)\n",
    "    \n",
    "    if not content:\n",
    "        logger.error(f\"Failed to fetch match details for: {match_id}\")\n",
    "        return {}\n",
    "    \n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    match_details = {\n",
    "        \"id\": match_id,\n",
    "        \"url\": match_url,\n",
    "        \"teams\": {},\n",
    "        \"score\": {},\n",
    "        \"events\": [],\n",
    "        \"stats\": {},\n",
    "        \"lineups\": {},\n",
    "        \"h2h\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Get teams\n",
    "        home_team_element = soup.select_one(\".duelParticipant__home\")\n",
    "        away_team_element = soup.select_one(\".duelParticipant__away\")\n",
    "        \n",
    "        if home_team_element and away_team_element:\n",
    "            match_details[\"teams\"][\"home\"] = home_team_element.text.strip()\n",
    "            match_details[\"teams\"][\"away\"] = away_team_element.text.strip()\n",
    "        \n",
    "        # Get score\n",
    "        score_element = soup.select_one(\".detailScore__wrapper\")\n",
    "        if score_element:\n",
    "            home_score = score_element.select_one(\".detailScore__home\")\n",
    "            away_score = score_element.select_one(\".detailScore__away\")\n",
    "            \n",
    "            if home_score and away_score:\n",
    "                match_details[\"score\"][\"full_time\"] = {\n",
    "                    \"home\": home_score.text.strip(),\n",
    "                    \"away\": away_score.text.strip()\n",
    "                }\n",
    "        \n",
    "        # Get half-time score\n",
    "        ht_score_element = soup.select_one(\".detailScore__status\")\n",
    "        if ht_score_element and \"HT\" in ht_score_element.text:\n",
    "            ht_score_match = re.search(r'\\((\\d+):(\\d+)\\)', ht_score_element.text)\n",
    "            if ht_score_match:\n",
    "                match_details[\"score\"][\"half_time\"] = {\n",
    "                    \"home\": ht_score_match.group(1),\n",
    "                    \"away\": ht_score_match.group(2)\n",
    "                }\n",
    "        \n",
    "        # Get match events (goals, cards, substitutions)\n",
    "        events_elements = soup.select(\".detailMS__incidentRow\")\n",
    "        \n",
    "        for event_element in events_elements:\n",
    "            event_type_element = event_element.select_one(\".detailMS__incidentType\")\n",
    "            event_time_element = event_element.select_one(\".detailMS__incidentTime\")\n",
    "            event_player_element = event_element.select_one(\".detailMS__incidentPlayer\")\n",
    "            \n",
    "            if event_type_element and event_time_element and event_player_element:\n",
    "                event_type = event_type_element.get(\"class\", [])\n",
    "                event_type = [cls for cls in event_type if \"icon\" in cls]\n",
    "                event_type = event_type[0].replace(\"icon-\", \"\") if event_type else \"unknown\"\n",
    "                \n",
    "                event_time = event_time_element.text.strip()\n",
    "                event_player = event_player_element.text.strip()\n",
    "                \n",
    "                event = {\n",
    "                    \"type\": event_type,\n",
    "                    \"time\": event_time,\n",
    "                    \"player\": event_player,\n",
    "                    \"team\": \"home\" if \"home\" in event_element.get(\"class\", []) else \"away\"\n",
    "                }\n",
    "                \n",
    "                match_details[\"events\"].append(event)\n",
    "        \n",
    "        # Get match statistics\n",
    "        stats_container = soup.select_one(\"#tab-statistics-0-statistic\")\n",
    "        if stats_container:\n",
    "            stat_items = stats_container.select(\".statRow\")\n",
    "            \n",
    "            for stat_item in stat_items:\n",
    "                stat_name_element = stat_item.select_one(\".statTextGroup\")\n",
    "                home_value_element = stat_item.select_one(\".statHomeValue\")\n",
    "                away_value_element = stat_item.select_one(\".statAwayValue\")\n",
    "                \n",
    "                if stat_name_element and home_value_element and away_value_element:\n",
    "                    stat_name = stat_name_element.text.strip().lower().replace(\" \", \"_\")\n",
    "                    home_value = home_value_element.text.strip()\n",
    "                    away_value = away_value_element.text.strip()\n",
    "                    \n",
    "                    match_details[\"stats\"][stat_name] = {\n",
    "                        \"home\": home_value,\n",
    "                        \"away\": away_value\n",
    "                    }\n",
    "        \n",
    "        # Get lineups\n",
    "        lineups_container = soup.select_one(\"#tab-lineups-0-team\")\n",
    "        if lineups_container:\n",
    "            home_lineup = lineups_container.select(\".lineups__playerHome .pl__name\")\n",
    "            away_lineup = lineups_container.select(\".lineups__playerAway .pl__name\")\n",
    "            \n",
    "            match_details[\"lineups\"][\"home\"] = [player.text.strip() for player in home_lineup]\n",
    "            match_details[\"lineups\"][\"away\"] = [player.text.strip() for player in away_lineup]\n",
    "        \n",
    "        # Get H2H (Head to Head)\n",
    "        h2h_container = soup.select_one(\"#tab-h2h-0\")\n",
    "        if h2h_container:\n",
    "            h2h_matches = h2h_container.select(\".h2h__match\")\n",
    "            \n",
    "            for h2h_match in h2h_matches:\n",
    "                home_team = h2h_match.select_one(\".h2h__homeParticipant\")\n",
    "                away_team = h2h_match.select_one(\".h2h__awayParticipant\")\n",
    "                result = h2h_match.select_one(\".h2h__result\")\n",
    "                date = h2h_match.select_one(\".h2h__date\")\n",
    "                \n",
    "                if home_team and away_team and result and date:\n",
    "                    h2h_item = {\n",
    "                        \"home_team\": home_team.text.strip(),\n",
    "                        \"away_team\": away_team.text.strip(),\n",
    "                        \"result\": result.text.strip(),\n",
    "                        \"date\": date.text.strip()\n",
    "                    }\n",
    "                    \n",
    "                    match_details[\"h2h\"].append(h2h_item)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing match details for {match_id}: {str(e)}\")\n",
    "    \n",
    "    return match_details\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "074cdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_live_matches(use_selenium: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get currently live matches.\n",
    "    \n",
    "    Args:\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries containing live match information\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/live/\"\n",
    "    content = get_page_content(url, use_selenium)\n",
    "    \n",
    "    if not content:\n",
    "        logger.error(\"Failed to fetch live matches\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    live_matches = []\n",
    "    \n",
    "    # This selector will need to be updated based on actual Flashscore HTML structure\n",
    "    match_elements = soup.select(\".event__match--live\")\n",
    "    \n",
    "    for element in match_elements:\n",
    "        try:\n",
    "            match_id = element.get(\"id\", \"\").replace(\"g_1_\", \"\")\n",
    "            \n",
    "            if not match_id:\n",
    "                continue\n",
    "            \n",
    "            home_team = element.select_one(\".event__participant--home\").text.strip()\n",
    "            away_team = element.select_one(\".event__participant--away\").text.strip()\n",
    "            \n",
    "            match_time_element = element.select_one(\".event__stage--block\")\n",
    "            match_time = match_time_element.text.strip() if match_time_element else \"\"\n",
    "            \n",
    "            score_element = element.select_one(\".event__scores\")\n",
    "            score = score_element.text.strip() if score_element else \"\"\n",
    "            \n",
    "            live_match = {\n",
    "                \"id\": match_id,\n",
    "                \"home_team\": home_team,\n",
    "                \"away_team\": away_team,\n",
    "                \"current_time\": match_time,\n",
    "                \"current_score\": score,\n",
    "                \"url\": f\"{BASE_URL}/match/{match_id}/\"\n",
    "            }\n",
    "            \n",
    "            live_matches.append(live_match)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing live match: {str(e)}\")\n",
    "    \n",
    "    logger.info(f\"Found {len(live_matches)} live matches\")\n",
    "    return live_matches\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b64064d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data(league_url: str, seasons: List[str], use_selenium: bool = True) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Get historical data for a league across multiple seasons.\n",
    "    \n",
    "    Args:\n",
    "        league_url: The URL of the league\n",
    "        seasons: List of season identifiers\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping seasons to lists of fixtures\n",
    "    \"\"\"\n",
    "    historical_data = {}\n",
    "    \n",
    "    for season in seasons:\n",
    "        logger.info(f\"Fetching historical data for season: {season}\")\n",
    "        fixtures = get_league_fixtures(league_url, season, use_selenium)\n",
    "        historical_data[season] = fixtures\n",
    "        \n",
    "        # Add a delay between seasons to avoid detection\n",
    "        random_delay(3.0, 6.0)\n",
    "    \n",
    "    return historical_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95e4650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data: Any, file_path: str):\n",
    "    \"\"\"\n",
    "    Save data to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        data: The data to save\n",
    "        file_path: The path to the output file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(f\"Data saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data to {file_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "def save_to_csv(data: List[Dict[str, Any]], file_path: str):\n",
    "    \"\"\"\n",
    "    Save data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        data: The data to save (list of dictionaries)\n",
    "        file_path: The path to the output file\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        logger.error(f\"No data to save to {file_path}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Get all unique keys from all dictionaries\n",
    "        fieldnames = set()\n",
    "        for item in data:\n",
    "            fieldnames.update(item.keys())\n",
    "        \n",
    "        with open(file_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=sorted(fieldnames))\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "        \n",
    "        logger.info(f\"Data saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data to {file_path}: {str(e)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "87057fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_league_scraper(league_url: str, output_format: str = \"json\", use_selenium: bool = True):\n",
    "    \"\"\"\n",
    "    Run the scraper for a specific league.\n",
    "    \n",
    "    Args:\n",
    "        league_url: The URL of the league\n",
    "        output_format: Output format (\"json\" or \"csv\")\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "    \"\"\"\n",
    "    league_id = league_url.split(\"/\")[-1]\n",
    "    \n",
    "    # Get all fixtures for the league\n",
    "    fixtures = get_league_fixtures(league_url, use_selenium=use_selenium)\n",
    "    \n",
    "    # Save fixtures\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"data/leagues/{league_id}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_file = f\"{output_dir}/fixtures_{timestamp}\"\n",
    "    \n",
    "    if output_format.lower() == \"json\":\n",
    "        save_to_json(fixtures, f\"{output_file}.json\")\n",
    "    elif output_format.lower() == \"csv\":\n",
    "        save_to_csv(fixtures, f\"{output_file}.csv\")\n",
    "    \n",
    "    # Get detailed information for each match\n",
    "    match_details = []\n",
    "    for fixture in fixtures[:10]:  # Limit to 10 matches for testing\n",
    "        match_id = fixture[\"id\"]\n",
    "        details = get_match_details(match_id, use_selenium=use_selenium)\n",
    "        match_details.append(details)\n",
    "\n",
    "        # Save individual match details\n",
    "        match_dir = f\"data/matches/{match_id}\"\n",
    "        os.makedirs(match_dir, exist_ok=True)\n",
    "        \n",
    "        if output_format.lower() == \"json\":\n",
    "            save_to_json(details, f\"{match_dir}/details.json\")\n",
    "        \n",
    "        # Add a delay between requests\n",
    "        random_delay()\n",
    "    \n",
    "    # Save all match details in one file\n",
    "    if output_format.lower() == \"json\":\n",
    "        save_to_json(match_details, f\"{output_dir}/match_details_{timestamp}.json\")\n",
    "    elif output_format.lower() == \"csv\":\n",
    "        # Flatten match details for CSV\n",
    "        flattened_details = []\n",
    "        for match in match_details:\n",
    "            flat_match = {\n",
    "                \"id\": match.get(\"id\", \"\"),\n",
    "                \"url\": match.get(\"url\", \"\"),\n",
    "                \"home_team\": match.get(\"teams\", {}).get(\"home\", \"\"),\n",
    "                \"away_team\": match.get(\"teams\", {}).get(\"away\", \"\"),\n",
    "                \"home_score\": match.get(\"score\", {}).get(\"full_time\", {}).get(\"home\", \"\"),\n",
    "                \"away_score\": match.get(\"score\", {}).get(\"full_time\", {}).get(\"away\", \"\"),\n",
    "                \"ht_home_score\": match.get(\"score\", {}).get(\"half_time\", {}).get(\"home\", \"\"),\n",
    "                \"ht_away_score\": match.get(\"score\", {}).get(\"half_time\", {}).get(\"away\", \"\")\n",
    "            }\n",
    "            \n",
    "            # Add statistics\n",
    "            for stat_name, stat_values in match.get(\"stats\", {}).items():\n",
    "                flat_match[f\"stat_{stat_name}_home\"] = stat_values.get(\"home\", \"\")\n",
    "                flat_match[f\"stat_{stat_name}_away\"] = stat_values.get(\"away\", \"\")\n",
    "            \n",
    "            flattened_details.append(flat_match)\n",
    "        \n",
    "        save_to_csv(flattened_details, f\"{output_dir}/match_details_{timestamp}.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a28be0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_live_scraper(interval: int = 60, duration: int = 3600, output_format: str = \"json\", use_selenium: bool = True):\n",
    "    \"\"\"\n",
    "    Run the scraper for live matches with periodic updates.\n",
    "    \n",
    "    Args:\n",
    "        interval: Update interval in seconds\n",
    "        duration: Total duration to run in seconds\n",
    "        output_format: Output format (\"json\" or \"csv\")\n",
    "        use_selenium: Whether to use Selenium or requests\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + duration\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        # Get current live matches\n",
    "        live_matches = get_live_matches(use_selenium=use_selenium)\n",
    "        \n",
    "        if live_matches:\n",
    "            # Save live matches\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_dir = \"data/live\"\n",
    "            output_file = f\"{output_dir}/live_matches_{timestamp}\"\n",
    "            \n",
    "            if output_format.lower() == \"json\":\n",
    "                save_to_json(live_matches, f\"{output_file}.json\")\n",
    "            elif output_format.lower() == \"csv\":\n",
    "                save_to_csv(live_matches, f\"{output_file}.csv\")\n",
    "            \n",
    "            # Get detailed information for each live match\n",
    "            for match in live_matches:\n",
    "                match_id = match[\"id\"]\n",
    "                details = get_match_details(match_id, use_selenium=use_selenium)\n",
    "                \n",
    "                match_dir = f\"{output_dir}/{match_id}\"\n",
    "                os.makedirs(match_dir, exist_ok=True)\n",
    "                \n",
    "                if output_format.lower() == \"json\":\n",
    "                    save_to_json(details, f\"{match_dir}/details_{timestamp}.json\")\n",
    "                \n",
    "                # Add a delay between requests\n",
    "                random_delay(1.0, 2.0)\n",
    "        \n",
    "        # Wait for the next update\n",
    "        time_to_sleep = max(0, interval - (time.time() - start_time) % interval)\n",
    "        logger.info(f\"Waiting {time_to_sleep:.2f} seconds for next update...\")\n",
    "        time.sleep(time_to_sleep)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b53cc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    \"\"\"Clean up resources.\"\"\"\n",
    "    global driver\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        driver = None\n",
    "\n",
    "\n",
    "def initialize(use_selenium: bool = True, headless: bool = True, chrome_driver_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Initialize the scraper.\n",
    "    \n",
    "    Args:\n",
    "        use_selenium: Whether to use Selenium for scraping\n",
    "        headless: Whether to run the browser in headless mode\n",
    "        chrome_driver_path: Path to the Chrome webdriver executable\n",
    "    \"\"\"\n",
    "    global driver, session\n",
    "    \n",
    "    # Set up directories\n",
    "    setup_directories()\n",
    "    \n",
    "    # Initialize driver or session\n",
    "    if use_selenium:\n",
    "        driver = setup_selenium_driver(headless, chrome_driver_path)\n",
    "    \n",
    "    session = init_session()\n",
    "    \n",
    "    logger.info(\"Scraper initialized\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "313dfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to demonstrate the Flashscore scraper.\"\"\"\n",
    "    # Check if running in IPython/Jupyter\n",
    "    is_jupyter = 'ipykernel' in sys.modules\n",
    "    \n",
    "    if not is_jupyter:\n",
    "        # Normal command line execution\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description=\"Flashscore Soccer Match Data Scraper\")\n",
    "        parser.add_argument(\"--no-selenium\", action=\"store_true\", help=\"Use requests/BeautifulSoup instead of Selenium\")\n",
    "        parser.add_argument(\"--headless\", action=\"store_true\", help=\"Run Selenium in headless mode\")\n",
    "        parser.add_argument(\"--chromedriver\", type=str, help=\"Path to ChromeDriver executable\")\n",
    "        parser.add_argument(\"--league\", type=str, help=\"League URL to scrape\")\n",
    "        parser.add_argument(\"--live\", action=\"store_true\", help=\"Scrape live matches\")\n",
    "        parser.add_argument(\"--interval\", type=int, default=60, help=\"Update interval for live matches in seconds\")\n",
    "        parser.add_argument(\"--duration\", type=int, default=3600, help=\"Duration to run live scraper in seconds\")\n",
    "        parser.add_argument(\"--format\", type=str, choices=[\"json\", \"csv\"], default=\"json\", help=\"Output format\")\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        # Default values for Jupyter\n",
    "        class Args:\n",
    "            no_selenium = False\n",
    "            headless = True\n",
    "            chromedriver = None\n",
    "            league = \"https://www.flashscore.com/football/brazil/serie-a-betano/\"\n",
    "            live = False\n",
    "            interval = 60\n",
    "            duration = 3600\n",
    "            format = \"csv\"\n",
    "        args = Args()\n",
    "\n",
    "    try:\n",
    "        initialize(\n",
    "            use_selenium=not args.no_selenium,\n",
    "            headless=args.headless,\n",
    "            chrome_driver_path=args.chromedriver\n",
    "        )\n",
    "        \n",
    "        if args.league:\n",
    "            run_league_scraper(args.league, args.format, use_selenium=not args.no_selenium)\n",
    "        elif args.live:\n",
    "            run_live_scraper(args.interval, args.duration, args.format, use_selenium=not args.no_selenium)\n",
    "        else:\n",
    "            leagues = get_available_leagues(use_selenium=not args.no_selenium)\n",
    "            print(\"Available leagues:\")\n",
    "            for i, league in enumerate(leagues):\n",
    "                print(f\"{i+1}. {league['name']} - {league['url']}\")\n",
    "            \n",
    "            if is_jupyter:\n",
    "                print(\"In Jupyter, please specify --league directly in function calls\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:10:53,113 - flashscore_scraper - INFO - Scraper initialized\n",
      "2025-05-04 18:11:10,806 - flashscore_scraper - INFO - Found 0 fixtures for league: https://www.flashscore.com/football/brazil/serie-a-betano/\n",
      "2025-05-04 18:11:10,819 - flashscore_scraper - ERROR - No data to save to data/leagues//fixtures_20250504_181110.csv\n",
      "2025-05-04 18:11:10,836 - flashscore_scraper - ERROR - No data to save to data/leagues//match_details_20250504_181110.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
